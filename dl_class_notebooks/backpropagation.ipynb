{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    A(Input)\n",
    "    F(Output)\n",
    "    \n",
    "    A --> B\n",
    "    B --> C\n",
    "    C --> D\n",
    "    D --> E\n",
    "    E --> F\n",
    "\n",
    "    \n",
    "    subgraph Hidden_Layer\n",
    "        B((Weighted Sum))\n",
    "        C((Sigmoid))\n",
    "    end\n",
    "    \n",
    "    subgraph Output_Layer\n",
    "        D((Weighted Sum))\n",
    "        E((Sigmoid))\n",
    "    end\n",
    "    \n",
    "    \n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style C fill:#f9f,stroke:#333,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple neural network with two layers, each one with one neuron. We'll use the sigmoid activation function for both neurons. Here's an example of backpropagation with this network:\n",
    "\n",
    "1. **Initialize the Network Parameters**:\n",
    "   Let's assume we have:\n",
    "   - Input data: $x = 0.5$\n",
    "   - Target output: $y_{\\text{target}} = 1$\n",
    "   - Random initial weights and biases for the hidden layer:\n",
    "     - $w_1 = 0.4$\n",
    "     - $b_1 = 0.5$\n",
    "   - Random initial weights and bias for the output layer:\n",
    "     - $w_2 = 0.8$\n",
    "     - $b_2 = -0.2$\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - Compute the weighted sum and activation for the hidden layer:\n",
    "   \n",
    "      $z_1 = w_1 \\cdot x + b_1 = 0.4 \\cdot 0.5 + 0.5 = 0.7$\n",
    "     \n",
    "     $a_1 = \\text{sigmoid}(z_1) = \\frac{1}{1 + e^{-z_1}} = \\frac{1}{1 + e^{-0.7}} \\approx 0.668$\n",
    "\n",
    "   - Compute the weighted sum and activation for the output layer:\n",
    "\n",
    "     $z_2 = w_2 \\cdot a_1 + b_2 = 0.8 \\cdot 0.668 - 0.2 = 0.333$\n",
    "     \n",
    "     $a_2 = \\text{sigmoid}(z_2) = \\frac{1}{1 + e^{-z_2}} = \\frac{1}{1 + e^{-0.333}} \\approx 0.583$\n",
    "\n",
    "3. **Compute Loss**:\n",
    "   - Compute the loss using a simple squared error loss function:\n",
    "     \n",
    "     $L = \\frac{1}{2}(y_{\\text{target}} - a_2)^2 = \\frac{1}{2}(1 - 0.583)^2 \\approx 0.083$\n",
    "\n",
    "4. **Backpropagation**:\n",
    "   - Compute the gradient of the loss with respect to the output layer activation:\n",
    "\n",
    "     $\\frac{\\partial L}{\\partial a_2} = -(y_{\\text{target}} - a_2) = -(1 - 0.583) = -0.417$\n",
    "\n",
    "   - Compute the gradient of the output layer activation with respect to its weighted sum:\n",
    "\n",
    "     $\\frac{\\partial a_2}{\\partial z_2} = a_2 \\cdot (1 - a_2) = 0.583 \\cdot (1 - 0.583) \\approx 0.244$\n",
    "   \n",
    "   - Compute the gradient of the loss with respect to the output layer weights and bias:\n",
    "\n",
    "     $\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} \\cdot a_1 = (-0.417) \\cdot 0.244 \\cdot 0.668 \\approx -0.067$\n",
    "     \n",
    "     $\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} = (-0.417) \\cdot 0.244 \\approx -0.102$\n",
    "\n",
    "   - Compute the gradient of the loss with respect to the hidden layer activation:\n",
    "     \n",
    "     $\\frac{\\partial L}{\\partial a_1} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} \\cdot w_2 = (-0.417) \\cdot 0.244 \\cdot 0.8 \\approx -0.080$\n",
    "\n",
    "   - Compute the gradient of the hidden layer activation with respect to its weighted sum:\n",
    "     \n",
    "     $\\frac{\\partial a_1}{\\partial z_1} = a_1 \\cdot (1 - a_1) = 0.668 \\cdot (1 - 0.668) \\approx 0.221$\n",
    "     \n",
    "   - Compute the gradient of the loss with respect to the hidden layer weights and bias:\n",
    "     \n",
    "     $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} \\cdot x = (-0.080) \\cdot 0.221 \\cdot 0.5 \\approx -0.009$\n",
    "     \n",
    "     $\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} = (-0.080) \\cdot 0.221 \\approx -0.018$\n",
    "\n",
    "5. **Update Weights and Bias**:\n",
    "\n",
    "   - Update the weights and bias using gradient descent (using $\\alpha = 0.1$ as the learning rate):\n",
    "\n",
    "   $w_1 \\leftarrow w_1 - \\alpha \\cdot \\frac{\\partial L}{\\partial w_1} = 0.4 - 0.1 \\cdot (-0.009) \\approx 0.401$\n",
    "   \n",
    "   $b_1 \\leftarrow b_1 - \\alpha \\cdot \\frac{\\partial L}{\\partial b_1} = 0.5 - 0.1 \\cdot (-0.018) \\approx 0.502$\n",
    "   \n",
    "   $w_2 \\leftarrow w_2 - \\alpha \\cdot \\frac{\\partial L}{\\partial w_2} = 0.8 - 0.1 \\cdot (-0.067) \\approx 0.807$\n",
    "   \n",
    "   $b_2 \\leftarrow b_2 - \\alpha \\cdot \\frac{\\partial L}{\\partial b_2} = -0.2 - 0.1 \\cdot (-0.102) \\approx -0.189$\n",
    "\n",
    "6. **Repeat**:\n",
    "\n",
    "   - Repeat steps 2-5 for a number of iterations or until convergence. \n",
    "   \n",
    "This example demonstrates a single iteration of forward pass, backpropagation, and weight update. In practice, you would typically perform multiple iterations of this process to train the network on a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
